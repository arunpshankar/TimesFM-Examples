{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Finetuning TimesFM on Stock Data**\n",
        "**IMPORTANT:** Run this notebook using Vertex AI Workbench \n",
        "\n",
        "#\n",
        "\n",
        "This notebook demonstrates how to finetune a [TimesFM](https://github.com/google-research/google-research/tree/master/times_fm) model (or a variant) on a custom time series dataset. We'll specifically show how to:\n",
        "\n",
        "1. Set up dependencies and prerequisites.\n",
        "2. Define a flexible framework for finetuning TimesFM models on your own time-series data.\n",
        "3. Download stock data (e.g., AAPL) with [yfinance](https://pypi.org/project/yfinance/).\n",
        "4. Optionally log training progress with [Weights & Biases](https://wandb.ai/site) (W&B).\n",
        "5. Visualize model predictions vs. ground truth.\n",
        "\n",
        "We'll use a simplified example for a single GPU, but the framework includes the capacity for distributed training if desired."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1. Prerequisites**\n",
        "\n",
        "Before running this notebook, ensure you have the following libraries installed:\n",
        "\n",
        "- `torch` (PyTorch)\n",
        "- `timesfm` (version 1.2.6 or later)\n",
        "- `yfinance` (for data fetching)\n",
        "- `wandb` (optional, for logging)\n",
        "\n",
        "Below are example commands to install these packages. Uncomment and run if you haven't installed them in your environment.\n",
        "\n",
        "> **Note:** If you already have these packages installed or prefer alternative versions, skip these cells or adjust as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Optional installs (uncomment if needed)\n",
        "# !pip install torch\n",
        "# !pip install timesfm[torch]\n",
        "# !pip install timesfm==1.2.6\n",
        "# !pip install yfinance\n",
        "# !pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Weights & Biases Setup**\n",
        "If you intend to log metrics to W&B, you can store your access token in an environment variable or specify it directly. In practice, you might run something like:\n",
        "\n",
        "```\n",
        "import os\n",
        "os.environ['WANDB_API_KEY'] = 'YOUR_WANDB_ACCESS_TOKEN'\n",
        "```\n",
        "\n",
        "You can also enter your W&B credentials when prompted.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **2. Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "from timesfm import TimesFm, TimesFmCheckpoint, TimesFmHparams\n",
        "from timesfm.pytorch_patched_decoder import PatchedTimeSeriesDecoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **3. Finetuning Framework**\n",
        "\n",
        "Below is a flexible framework for training or finetuning a TimesFM model on custom time-series data. The code supports single- and multi-GPU (distributed) training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MetricsLogger(ABC):\n",
        "    \"\"\"Abstract base class for logging metrics during training.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def log_metrics(self, metrics: Dict[str, Any], step: Optional[int] = None) -> None:\n",
        "        \"\"\"Log metrics to the specified backend.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def close(self) -> None:\n",
        "        \"\"\"Clean up any resources used by the logger.\"\"\"\n",
        "        pass\n",
        "\n",
        "class WandBLogger(MetricsLogger):\n",
        "    \"\"\"Weights & Biases implementation of metrics logging.\"\"\"\n",
        "\n",
        "    def __init__(self, project: str, config: Dict[str, Any], rank: int = 0):\n",
        "        self.rank = rank\n",
        "        if rank == 0:\n",
        "            wandb.init(project=project, config=config)\n",
        "\n",
        "    def log_metrics(self, metrics: Dict[str, Any], step: Optional[int] = None) -> None:\n",
        "        if self.rank == 0:\n",
        "            wandb.log(metrics, step=step)\n",
        "\n",
        "    def close(self) -> None:\n",
        "        if self.rank == 0:\n",
        "            wandb.finish()\n",
        "\n",
        "class DistributedManager:\n",
        "    \"\"\"Manages distributed training setup and cleanup.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        world_size: int,\n",
        "        rank: int,\n",
        "        master_addr: str = \"localhost\",\n",
        "        master_port: str = \"12358\",\n",
        "        backend: str = \"nccl\",\n",
        "    ):\n",
        "        self.world_size = world_size\n",
        "        self.rank = rank\n",
        "        self.master_addr = master_addr\n",
        "        self.master_port = master_port\n",
        "        self.backend = backend\n",
        "\n",
        "    def setup(self) -> None:\n",
        "        os.environ[\"MASTER_ADDR\"] = self.master_addr\n",
        "        os.environ[\"MASTER_PORT\"] = self.master_port\n",
        "\n",
        "        if not dist.is_initialized():\n",
        "            dist.init_process_group(backend=self.backend, world_size=self.world_size, rank=self.rank)\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        if dist.is_initialized():\n",
        "            dist.destroy_process_group()\n",
        "\n",
        "@dataclass\n",
        "class FinetuningConfig:\n",
        "    \"\"\"Configuration for model training.\"\"\"\n",
        "    batch_size: int = 32\n",
        "    num_epochs: int = 20\n",
        "    learning_rate: float = 1e-4\n",
        "    weight_decay: float = 0.01\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    distributed: bool = False\n",
        "    gpu_ids: List[int] = field(default_factory=lambda: [0])\n",
        "    master_port: str = \"12358\"\n",
        "    master_addr: str = \"localhost\"\n",
        "    use_wandb: bool = False\n",
        "    wandb_project: str = \"timesfm-finetuning\"\n",
        "\n",
        "class TimesFMFinetuner:\n",
        "    \"\"\"Handles model training and validation for TimesFM.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        config: FinetuningConfig,\n",
        "        rank: int = 0,\n",
        "        loss_fn: Optional[Callable] = None,\n",
        "        logger: Optional[logging.Logger] = None,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.rank = rank\n",
        "        self.logger = logger or logging.getLogger(__name__)\n",
        "        self.device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.loss_fn = loss_fn or (lambda x, y: torch.mean((x - y.squeeze(-1)) ** 2))\n",
        "\n",
        "        if config.use_wandb:\n",
        "            self.metrics_logger = WandBLogger(config.wandb_project, config.__dict__, rank)\n",
        "\n",
        "        if config.distributed:\n",
        "            self.dist_manager = DistributedManager(\n",
        "                world_size=len(config.gpu_ids),\n",
        "                rank=rank,\n",
        "                master_addr=config.master_addr,\n",
        "                master_port=config.master_port,\n",
        "            )\n",
        "            self.dist_manager.setup()\n",
        "            self.model = self._setup_distributed_model()\n",
        "\n",
        "    def _setup_distributed_model(self) -> nn.Module:\n",
        "        self.model = self.model.to(self.device)\n",
        "        return DDP(\n",
        "            self.model,\n",
        "            device_ids=[self.config.gpu_ids[self.rank]],\n",
        "            output_device=self.config.gpu_ids[self.rank]\n",
        "        )\n",
        "\n",
        "    def _create_dataloader(self, dataset: Dataset, is_train: bool) -> DataLoader:\n",
        "        if self.config.distributed:\n",
        "            sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                dataset,\n",
        "                num_replicas=len(self.config.gpu_ids),\n",
        "                rank=dist.get_rank(),\n",
        "                shuffle=is_train\n",
        "            )\n",
        "        else:\n",
        "            sampler = None\n",
        "\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.config.batch_size,\n",
        "            shuffle=(is_train and not self.config.distributed),\n",
        "            sampler=sampler,\n",
        "        )\n",
        "\n",
        "    def _process_batch(self, batch: List[torch.Tensor]) -> tuple:\n",
        "        x_context, x_padding, freq, x_future = [t.to(self.device, non_blocking=True) for t in batch]\n",
        "\n",
        "        predictions = self.model(x_context, x_padding.float(), freq)\n",
        "        predictions_mean = predictions[..., 0]\n",
        "        last_patch_pred = predictions_mean[:, -1, :]\n",
        "\n",
        "        loss = self.loss_fn(last_patch_pred, x_future.squeeze(-1))\n",
        "        return loss, predictions\n",
        "\n",
        "    def _train_epoch(self, train_loader: DataLoader, optimizer: torch.optim.Optimizer) -> float:\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            loss, _ = self._process_batch(batch)\n",
        "\n",
        "            if self.config.distributed:\n",
        "                losses = [torch.zeros_like(loss) for _ in range(dist.get_world_size())]\n",
        "                dist.all_gather(losses, loss)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(train_loader)\n",
        "\n",
        "    def _validate(self, val_loader: DataLoader) -> float:\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                loss, _ = self._process_batch(batch)\n",
        "\n",
        "                if self.config.distributed:\n",
        "                    losses = [torch.zeros_like(loss) for _ in range(dist.get_world_size())]\n",
        "                    dist.all_gather(losses, loss)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(val_loader)\n",
        "\n",
        "    def finetune(self, train_dataset: Dataset, val_dataset: Dataset) -> Dict[str, Any]:\n",
        "        self.model = self.model.to(self.device)\n",
        "        train_loader = self._create_dataloader(train_dataset, is_train=True)\n",
        "        val_loader = self._create_dataloader(val_dataset, is_train=False)\n",
        "\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=self.config.learning_rate,\n",
        "            weight_decay=self.config.weight_decay\n",
        "        )\n",
        "\n",
        "        history = {\"train_loss\": [], \"val_loss\": [], \"learning_rate\": []}\n",
        "\n",
        "        self.logger.info(f\"Starting training for {self.config.num_epochs} epochs...\")\n",
        "        self.logger.info(f\"Training samples: {len(train_dataset)}\")\n",
        "        self.logger.info(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "        try:\n",
        "            for epoch in range(self.config.num_epochs):\n",
        "                train_loss = self._train_epoch(train_loader, optimizer)\n",
        "                val_loss = self._validate(val_loader)\n",
        "                current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "                metrics = {\n",
        "                    \"train_loss\": train_loss,\n",
        "                    \"val_loss\": val_loss,\n",
        "                    \"learning_rate\": current_lr,\n",
        "                    \"epoch\": epoch + 1,\n",
        "                }\n",
        "\n",
        "                if self.config.use_wandb:\n",
        "                    self.metrics_logger.log_metrics(metrics)\n",
        "\n",
        "                history[\"train_loss\"].append(train_loss)\n",
        "                history[\"val_loss\"].append(val_loss)\n",
        "                history[\"learning_rate\"].append(current_lr)\n",
        "\n",
        "                if self.rank == 0:\n",
        "                    self.logger.info(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            self.logger.info(\"Training interrupted by user\")\n",
        "\n",
        "        if self.config.distributed:\n",
        "            self.dist_manager.cleanup()\n",
        "\n",
        "        if self.config.use_wandb:\n",
        "            self.metrics_logger.close()\n",
        "\n",
        "        return {\"history\": history}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **4. Dataset Preparation**\n",
        "\n",
        "We define a simple `TimeSeriesDataset` that creates sliding-window samples from a single time series. This structure can be adapted for more complex multi-variate datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"Dataset for time series data compatible with TimesFM.\"\"\"\n",
        "\n",
        "    def __init__(self, series: np.ndarray, context_length: int, horizon_length: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            series: Time series data.\n",
        "            context_length: Number of past timesteps to use as input.\n",
        "            horizon_length: Number of future timesteps to predict.\n",
        "        \"\"\"\n",
        "        self.series = series\n",
        "        self.context_length = context_length\n",
        "        self.horizon_length = horizon_length\n",
        "        self._prepare_samples()\n",
        "\n",
        "    def _prepare_samples(self) -> None:\n",
        "        self.samples = []\n",
        "        total_length = self.context_length + self.horizon_length\n",
        "\n",
        "        for start_idx in range(0, len(self.series) - total_length + 1):\n",
        "            end_idx = start_idx + self.context_length\n",
        "            x_context = self.series[start_idx:end_idx]\n",
        "            x_future = self.series[end_idx : end_idx + self.horizon_length]\n",
        "            self.samples.append((x_context, x_future))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        x_context, x_future = self.samples[index]\n",
        "        x_context = torch.tensor(x_context, dtype=torch.float32)\n",
        "        x_future = torch.tensor(x_future, dtype=torch.float32)\n",
        "\n",
        "        # TimesFM expects a certain input format with freq, etc.\n",
        "        input_padding = torch.zeros_like(x_context)\n",
        "        freq = torch.zeros(1, dtype=torch.long)\n",
        "\n",
        "        return x_context, input_padding, freq, x_future\n",
        "\n",
        "def prepare_datasets(\n",
        "    series: np.ndarray,\n",
        "    context_length: int,\n",
        "    horizon_length: int,\n",
        "    train_split: float = 0.8\n",
        ") -> Tuple[Dataset, Dataset]:\n",
        "    \"\"\"\n",
        "    Prepare training and validation datasets from time series data.\n",
        "\n",
        "    Args:\n",
        "        series: Input time series data.\n",
        "        context_length: Number of past timesteps to use.\n",
        "        horizon_length: Number of future timesteps to predict.\n",
        "        train_split: Fraction of data to use for training.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_dataset, val_dataset).\n",
        "    \"\"\"\n",
        "    train_size = int(len(series) * train_split)\n",
        "    train_data = series[:train_size]\n",
        "    val_data = series[train_size:]\n",
        "\n",
        "    train_dataset = TimeSeriesDataset(train_data, context_length=context_length, horizon_length=horizon_length)\n",
        "    val_dataset = TimeSeriesDataset(val_data, context_length=context_length, horizon_length=horizon_length)\n",
        "    return train_dataset, val_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **5. Model Creation**\n",
        "\n",
        "We'll define a helper function that downloads the official `google/timesfm-2.0-500m-pytorch` checkpoint (using [Hugging Face Hub](https://huggingface.co/)) and constructs a [PatchedTimeSeriesDecoder](https://github.com/google-research/google-research/tree/master/times_fm) model. \n",
        "\n",
        "This function allows you to toggle whether to load weights from the checkpoint. By default, it loads them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from os import path\n",
        "\n",
        "def get_model(load_weights: bool = True):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    repo_id = \"google/timesfm-2.0-500m-pytorch\"\n",
        "    hparams = TimesFmHparams(\n",
        "        backend=device,\n",
        "        per_core_batch_size=32,\n",
        "        horizon_len=128,\n",
        "        num_layers=50,\n",
        "        use_positional_embedding=False,\n",
        "        context_len=192,\n",
        "    )\n",
        "\n",
        "    tfm = TimesFm(hparams=hparams, checkpoint=TimesFmCheckpoint(huggingface_repo_id=repo_id))\n",
        "    model = PatchedTimeSeriesDecoder(tfm._model_config)\n",
        "\n",
        "    if load_weights:\n",
        "        checkpoint_path = path.join(snapshot_download(repo_id), \"torch_model.ckpt\")\n",
        "        loaded_checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
        "        model.load_state_dict(loaded_checkpoint)\n",
        "\n",
        "    return model, hparams, tfm._model_config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **6. Utility Functions for Plotting & Data**\n",
        "\n",
        "Below are:\n",
        "1. A utility function to plot predictions vs. ground truth.\n",
        "2. A function `get_data` which downloads Apple (AAPL) stock prices from `yfinance`, then prepares training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_predictions(\n",
        "    model: nn.Module,\n",
        "    val_dataset: Dataset,\n",
        "    save_path: Optional[str] = \"predictions.png\",\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot model predictions against ground truth for a single batch of validation data.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Take the first sample in the validation dataset\n",
        "    x_context, x_padding, freq, x_future = val_dataset[0]\n",
        "    x_context = x_context.unsqueeze(0)\n",
        "    x_padding = x_padding.unsqueeze(0)\n",
        "    freq = freq.unsqueeze(0)\n",
        "    x_future = x_future.unsqueeze(0)\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    x_context = x_context.to(device)\n",
        "    x_padding = x_padding.to(device)\n",
        "    freq = freq.to(device)\n",
        "    x_future = x_future.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(x_context, x_padding.float(), freq)\n",
        "        predictions_mean = predictions[..., 0]  # shape [B, N, horizon_len]\n",
        "        last_patch_pred = predictions_mean[:, -1, :]  # shape [B, horizon_len]\n",
        "\n",
        "    context_vals = x_context[0].cpu().numpy()\n",
        "    future_vals = x_future[0].cpu().numpy()\n",
        "    pred_vals = last_patch_pred[0].cpu().numpy()\n",
        "\n",
        "    context_len = len(context_vals)\n",
        "    horizon_len = len(future_vals)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(range(context_len), context_vals, label=\"Historical Data\", color=\"blue\", linewidth=2)\n",
        "    plt.plot(\n",
        "        range(context_len, context_len + horizon_len),\n",
        "        future_vals,\n",
        "        label=\"Ground Truth\",\n",
        "        color=\"green\",\n",
        "        linestyle=\"--\",\n",
        "        linewidth=2,\n",
        "    )\n",
        "    plt.plot(\n",
        "        range(context_len, context_len + horizon_len),\n",
        "        pred_vals,\n",
        "        label=\"Prediction\",\n",
        "        color=\"red\",\n",
        "        linewidth=2,\n",
        "    )\n",
        "\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.title(\"TimesFM Predictions vs Ground Truth\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def get_data(context_len: int, horizon_len: int) -> Tuple[Dataset, Dataset]:\n",
        "    \"\"\"\n",
        "    Download AAPL stock data from yfinance, then split into train/val sets.\n",
        "    \"\"\"\n",
        "    df = yf.download(\"AAPL\", start=\"2010-01-01\", end=\"2019-01-01\")\n",
        "    time_series = df[\"Close\"].values\n",
        "\n",
        "    train_dataset, val_dataset = prepare_datasets(\n",
        "        series=time_series,\n",
        "        context_length=context_len,\n",
        "        horizon_length=horizon_len,\n",
        "        train_split=0.8,\n",
        "    )\n",
        "\n",
        "    print(\"Created datasets:\")\n",
        "    print(f\"- Training samples: {len(train_dataset)}\")\n",
        "    print(f\"- Validation samples: {len(val_dataset)}\")\n",
        "    return train_dataset, val_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **7. Single-GPU Finetuning Example**\n",
        "\n",
        "Below is a convenience function that:\n",
        "1. Builds the model.\n",
        "2. Creates the training and validation datasets.\n",
        "3. Initializes the finetuner.\n",
        "4. Trains for a few epochs.\n",
        "5. Logs results (optional via W&B).\n",
        "6. Plots predictions vs. ground truth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def single_gpu_example():\n",
        "    # 1. Create model & load checkpoint\n",
        "    model, hparams, tfm_config = get_model(load_weights=True)\n",
        "\n",
        "    # 2. Define finetuning config (feel free to adjust epochs, batch_size, etc.)\n",
        "    config = FinetuningConfig(\n",
        "        batch_size=256,\n",
        "        num_epochs=5,\n",
        "        learning_rate=1e-4,\n",
        "        use_wandb=True,  # set to False if you don't want to log to W&B\n",
        "        wandb_project=\"timesfm-finetuning\",\n",
        "    )\n",
        "\n",
        "    # 3. Prepare data\n",
        "    train_dataset, val_dataset = get_data(128, tfm_config.horizon_len)\n",
        "\n",
        "    # 4. Finetuner\n",
        "    finetuner = TimesFMFinetuner(model, config)\n",
        "\n",
        "    # 5. Train\n",
        "    print(\"\\nStarting finetuning...\")\n",
        "    results = finetuner.finetune(train_dataset=train_dataset, val_dataset=val_dataset)\n",
        "\n",
        "    print(\"\\nFinetuning completed!\")\n",
        "    print(f\"Training history: {len(results['history']['train_loss'])} epochs\")\n",
        "\n",
        "    # 6. Plot predictions\n",
        "    plot_predictions(\n",
        "        model=model,\n",
        "        val_dataset=val_dataset,\n",
        "        save_path=\"timesfm_predictions.png\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **8. Run the Example**\n",
        "\n",
        "Execute the cell below to run the entire pipeline on a single GPU. If you haven't configured W&B or you don't have an account, simply set `use_wandb=False` in the `FinetuningConfig`.\n",
        "\n",
        "> **Note**: This may download a large checkpoint (2GB) from the Hugging Face Hub on first run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to run the single-GPU finetuning example.\n",
        "# single_gpu_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Conclusion**\n",
        "This notebook has shown how to:\n",
        "- Install and prepare a TimesFM model.\n",
        "- Build a training and validation pipeline with logging.\n",
        "- Finetune on custom time series data.\n",
        "- Visualize the model’s predictions.\n",
        "\n",
        "Adapt or extend this workflow for your own dataset and tasks!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
